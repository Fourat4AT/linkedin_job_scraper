{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "linked in scrapper",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "Y7Ajjl8IAB0y"
      },
      "outputs": [],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-06T00:20:17.637435Z",
          "iopub.execute_input": "2025-11-06T00:20:17.639876Z"
        },
        "id": "Y68tMcKYAB00"
      },
      "outputs": [],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.parse\n",
        "import gradio as gr  # gradio to build the web app\n",
        "import requests      # to fetch web pages\n",
        "from bs4 import BeautifulSoup  # to parse html\n",
        "import pandas as pd  # to handle data tables\n",
        "from datetime import datetime  # to add dates\n",
        "import time          # for adding delays in requests\n",
        "\n",
        "# flair tools for AI skill detection\n",
        "from flair.models import SequenceTagger\n",
        "from flair.data import Sentence\n",
        "\n",
        "import threading      # for smooth background tasks\n",
        "import os             # to save files\n",
        "import random         # for random delays\n",
        "\n",
        "# retry tools to handle web errors\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-06T00:11:26.932662Z",
          "iopub.execute_input": "2025-11-06T00:11:26.933746Z",
          "iopub.status.idle": "2025-11-06T00:11:26.960648Z",
          "shell.execute_reply.started": "2025-11-06T00:11:26.933708Z",
          "shell.execute_reply": "2025-11-06T00:11:26.958824Z"
        },
        "id": "JQvDIDplAB02"
      },
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOADING MODAL**"
      ],
      "metadata": {
        "id": "LqLdf69UAB03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flair_model = SequenceTagger.load(\"kaliani/flair-ner-skill\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-06T00:11:26.963212Z",
          "iopub.status.idle": "2025-11-06T00:11:26.963556Z",
          "shell.execute_reply.started": "2025-11-06T00:11:26.963414Z",
          "shell.execute_reply": "2025-11-06T00:11:26.963426Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GWjLcIEAB05",
        "outputId": "b946fffe-92da-4299-f0d3-78c79ad1b60a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-09 13:05:09,255 SequenceTagger predicts: Dictionary with 7 tags: O, S-SKILL, B-SKILL, E-SKILL, I-SKILL, <START>, <STOP>\n"
          ]
        }
      ],
      "execution_count": 30
    },
    {
      "cell_type": "code",
      "source": [
        "# Map experience levels to LinkedIn URL codes\n",
        "experience_level_mapping = {\n",
        "    \"Internship\": \"f_E=1\",\n",
        "    \"Entry level\": \"f_E=2\",\n",
        "    \"Associate\": \"f_E=3\",\n",
        "    \"Mid-Senior level\": \"f_E=4\"\n",
        "}\n",
        "\n",
        "# Map work type to LinkedIn URL codes\n",
        "work_type_mapping = {\n",
        "    \"On-site\": \"f_WT=1\",\n",
        "    \"Hybrid\":  \"f_WT=3\",   # ← WAS 2\n",
        "    \"Remote\":  \"f_WT=2\"    # ← WAS 3\n",
        "}\n",
        "\n",
        "# Map time filters to LinkedIn URL codes\n",
        "time_filter_mapping = {\n",
        "    \"Past 24 hours\": \"f_TPR=r86400\",\n",
        "    \"Past week\": \"f_TPR=r604800\",\n",
        "    \"Past month\": \"f_TPR=r2592000\"\n",
        "}\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-06T00:11:26.967387Z",
          "iopub.status.idle": "2025-11-06T00:11:26.967802Z",
          "shell.execute_reply.started": "2025-11-06T00:11:26.967602Z",
          "shell.execute_reply": "2025-11-06T00:11:26.967621Z"
        },
        "id": "-xpXaoAoAB06"
      },
      "outputs": [],
      "execution_count": 31
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to find skills in a text description\n",
        "def get_skills(text):\n",
        "    # Turn the input text into Flair's Sentence object\n",
        "    sentence = Sentence(text)\n",
        "\n",
        "    # Use the Flair NER model to detect skills in the sentence\n",
        "    flair_model.predict(sentence)\n",
        "\n",
        "    # Extract the detected skills from the sentence\n",
        "    skills = [entity.text for entity in sentence.get_spans(\"ner\")]\n",
        "\n",
        "    return skills\n",
        "\n",
        "# Example usage:\n",
        "description = \"I have experience in Python, TensorFlow, and data analysis.\"\n",
        "found_skills = get_skills(description)\n",
        "print(found_skills)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-07T22:09:44.181452Z",
          "iopub.execute_input": "2025-11-07T22:09:44.181693Z",
          "iopub.status.idle": "2025-11-07T22:09:44.2782Z",
          "shell.execute_reply.started": "2025-11-07T22:09:44.181674Z",
          "shell.execute_reply": "2025-11-07T22:09:44.276983Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n4xjGNvAB07",
        "outputId": "55345dac-df36-4c59-e7fa-fe93e0beb0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Python', 'data analysis', '.']\n"
          ]
        }
      ],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": [
        "class ScraperManager:\n",
        "    # Setup when class starts\n",
        "    def __init__(self):\n",
        "        # Flag to stop scraping\n",
        "        self.stop_event = threading.Event()\n",
        "\n",
        "        # Empty table for job data\n",
        "        self.current_df = pd.DataFrame()\n",
        "\n",
        "        # Lock to avoid data mix-ups\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    # Reset for new scrape\n",
        "    def reset(self):\n",
        "        # Clear stop flag\n",
        "        self.stop_event.clear()\n",
        "\n",
        "        # Clear job table\n",
        "        self.current_df = pd.DataFrame()\n",
        "\n",
        "    # Add one job to table\n",
        "    def add_job(self, job_data):\n",
        "        # Lock to keep data safe\n",
        "        with self.lock:\n",
        "            # Make job into tiny table\n",
        "            new_df = pd.DataFrame([job_data])\n",
        "\n",
        "            # Add to main table\n",
        "            self.current_df = pd.concat([self.current_df, new_df], ignore_index=True)\n",
        "\n",
        "# -------------------------------\n",
        "# Create manager instance\n",
        "# -------------------------------\n",
        "scraper_manager = ScraperManager()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "wdhZPeQ1AB08"
      },
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": [
        "#define function to save jobs\n",
        "def save_csv(df, filename=\"jobs\"):\n",
        "    try:\n",
        "        #make folder for files\n",
        "        os.makedirs(\"saved_jobs\", exist_ok=True)\n",
        "\n",
        "        #set default name with time stamp\n",
        "        if not filename:\n",
        "            filename = f\"jobs_{int(time.time())}\"\n",
        "\n",
        "        #build file path\n",
        "        full_path = f\"saved_jobs/{filename}.csv\"\n",
        "\n",
        "        #save table to csv\n",
        "        df.to_csv(full_path, index=False)\n",
        "\n",
        "        #Confirm save worked\n",
        "        return f\"Saved to {full_path}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        #show error if save fails\n",
        "        return f\"Save error: {str(e)}\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "g3IskQ5IAB09"
      },
      "outputs": [],
      "execution_count": 34
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proecss JOB FUNCTION**"
      ],
      "metadata": {
        "id": "0XLi9A5yAB0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_job(job, work_type, exp_level, position):\n",
        "    try:\n",
        "        # find job title\n",
        "        title_element = job.find('h3', class_='base-search-card__title')\n",
        "        # find company name\n",
        "        company_element = job.find('a', class_='hidden-nested-link')\n",
        "        # find location\n",
        "        loc_element = job.find('span', class_='job-search-card__location')\n",
        "        # find job link\n",
        "        link_element = job.find('a', class_='base-card__full-link')\n",
        "\n",
        "        # check all data exists\n",
        "        if not all([title_element, company_element, loc_element, link_element]):\n",
        "            return None\n",
        "\n",
        "        # clean title text\n",
        "        title = title_element.text.strip()\n",
        "        # clean company text\n",
        "        company = company_element.text.strip()\n",
        "        # clean location text\n",
        "        loc = loc_element.text.strip()\n",
        "        # clean link (remove extra bits)\n",
        "        link = link_element['href'].split('?')[0]\n",
        "\n",
        "        # setup web session with retries\n",
        "        session = requests.Session()\n",
        "        retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "        # make request to job link\n",
        "        desc = \"Description not available \"\n",
        "        # empty skills list\n",
        "        skills = []\n",
        "\n",
        "        try:\n",
        "            time.sleep(random.uniform(2,5))\n",
        "            response = session.get(\n",
        "                link,\n",
        "                headers={\n",
        "                    'User-Agent': random.choice([\n",
        "                        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)',\n",
        "                        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko)',\n",
        "                        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96'\n",
        "                    ]),\n",
        "                    \"Accept-Language\": \"en-us,en;q=0.9\"\n",
        "                },\n",
        "                timeout=10\n",
        "            )\n",
        "\n",
        "            # parse job page\n",
        "            job_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # list of places to find description\n",
        "            description_selectors = [\n",
        "                'div.description__text',\n",
        "                'div.show-more-less-html__markup',\n",
        "                'div.core-section-container__content',\n",
        "                'section.core-section-container'\n",
        "            ]\n",
        "\n",
        "            # try each description spot\n",
        "            for selector in description_selectors:\n",
        "                desc_element = job_soup.select_one(selector)\n",
        "                if desc_element:\n",
        "                    # clean description text\n",
        "                    desc = desc_element.get_text('\\n').strip()\n",
        "                    # find skills with AI\n",
        "                    skills = get_skills(desc)\n",
        "                    break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {link}: {str(e)}\")\n",
        "            # Still return what we have\n",
        "\n",
        "        # return job details\n",
        "        return {\n",
        "            \"Position\": position,\n",
        "            \"Date\": datetime.now().strftime('%Y-%m-%d'),\n",
        "            \"Work type\": work_type,\n",
        "            \"Level\": exp_level,\n",
        "            \"Title\": title,\n",
        "            \"Company\": company,\n",
        "            \"Location\": loc,\n",
        "            \"Link\": f\"[{link}]({link})\",\n",
        "            \"Description\": desc,\n",
        "            \"Skills\": \", \".join(skills[:5]) if skills else \"No skills detected\"\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        # log error if job fails\n",
        "        print(f\"Error packing job card: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "trusted": true,
        "id": "9s3PsB9rAB0-"
      },
      "outputs": [],
      "execution_count": 35
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SCRAPE JOB FUNCTION******"
      ],
      "metadata": {
        "id": "nYNjdgQHAB1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== SCRAPE JOBS (FIXED URL) =====\n",
        "def scrape_jobs(location, position, work_types, exp_levels, time_filter):\n",
        "    session = requests.Session()\n",
        "    retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "    for work_type in work_types:\n",
        "        for exp_level in exp_levels:\n",
        "            if scraper_manager.stop_event.is_set():\n",
        "                return\n",
        "\n",
        "            try:\n",
        "                # FIX 4: Proper URL encoding + safe params\n",
        "                pos_enc = urllib.parse.quote(position)\n",
        "                loc_enc = urllib.parse.quote(location)\n",
        "                base_url = f\"https://www.linkedin.com/jobs/search/?keywords={pos_enc}&location={loc_enc}\"\n",
        "                base_url += f\"&{work_type_mapping[work_type]}\" if work_type in work_type_mapping else \"\"\n",
        "                base_url += f\"&{experience_level_mapping[exp_level]}\" if exp_level in experience_level_mapping else \"\"\n",
        "                base_url += f\"&{time_filter_mapping[time_filter]}\" if time_filter in time_filter_mapping else \"\"\n",
        "                base_url += \"&sortBy=DD\"\n",
        "\n",
        "                # Get total\n",
        "                try:\n",
        "                    resp = session.get(base_url, timeout=15)\n",
        "                    soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "                    total_el = soup.find('span', class_='results-context-header__job-count')\n",
        "                    total_jobs = int(total_el.text.replace(',', '')) if total_el else 25\n",
        "                except:\n",
        "                    total_jobs = 25\n",
        "                total_jobs = min(total_jobs, 50)\n",
        "\n",
        "                for start in range(0, total_jobs, 25):\n",
        "                    if scraper_manager.stop_event.is_set():\n",
        "                        return\n",
        "                    time.sleep(random.uniform(2, 4))\n",
        "                    url = f\"{base_url}&start={start}\"\n",
        "                    try:\n",
        "                        resp = session.get(url, timeout=10)\n",
        "                        soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "                        cards = soup.find_all('div', class_='base-card')\n",
        "                        random.shuffle(cards)\n",
        "                        for card in cards:\n",
        "                            if scraper_manager.stop_event.is_set():\n",
        "                                return\n",
        "                            data = process_job(card, work_type, exp_level, position)\n",
        "                            if data:\n",
        "                                scraper_manager.add_job(data)\n",
        "                                # FIX 5: Yield to update GUI\n",
        "                                yield\n",
        "                    except Exception as e:\n",
        "                        print(f\"Page failed: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Combo failed: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "aE0p-7gmAB1A"
      },
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "k-k9_hZqAB1B"
      },
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "code",
      "source": [
        "def run_scraper(cities, states, positions_input, work_types, exp_levels, time_filter):\n",
        "    # Clean inputs\n",
        "    locations = [f\"{c.strip()}, {s.strip()}\"\n",
        "                 for c, s in zip(cities.split(','), states.split(','))\n",
        "                 if c.strip() and s.strip()]\n",
        "    positions = [p.strip() for p in positions_input.split(',') if p.strip()]\n",
        "\n",
        "    # Reset\n",
        "    scraper_manager.reset()\n",
        "\n",
        "    # Worker thread\n",
        "    def worker():\n",
        "        for loc in locations:\n",
        "            for pos in positions:\n",
        "                if scraper_manager.stop_event.is_set():\n",
        "                    return\n",
        "                # Consume generator\n",
        "                for _ in scrape_jobs(loc, pos, work_types, exp_levels, time_filter):\n",
        "                    pass\n",
        "\n",
        "    thread = threading.Thread(target=worker)\n",
        "    thread.start()\n",
        "\n",
        "    # LIVE UPDATE LOOP\n",
        "    while thread.is_alive():\n",
        "        time.sleep(0.5)\n",
        "        with scraper_manager.lock:\n",
        "            yield \"scraping in progress ...\", scraper_manager.current_df\n",
        "\n",
        "    # FINAL\n",
        "    status = \"scraping completed\" if not scraper_manager.stop_event.is_set() else \"scraping stopped\"\n",
        "    yield status, scraper_manager.current_df"
      ],
      "metadata": {
        "trusted": true,
        "id": "gFw6leCYAB1B"
      },
      "outputs": [],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as app:\n",
        "    # Add title\n",
        "    gr.Markdown(\"\"\"\n",
        "    <div style='text-align:center; color:#f67d3c; font-size:2em; font-weight:bold; margin:20px 0; padding:10px'>\n",
        "        AI-Powered LinkedIn Scraper\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            # Input for cities\n",
        "            cities = gr.Textbox(label=\"Cities (comma-separated)\")\n",
        "\n",
        "            # Input for states\n",
        "            states = gr.Textbox(label=\"States / Countries (comma-separated)\")\n",
        "\n",
        "            # Input for positions\n",
        "            positions = gr.Textbox(label=\"Positions (comma-separated)\")\n",
        "\n",
        "            # checkbox for work types\n",
        "            work_types = gr.CheckboxGroup(\n",
        "                list(work_type_mapping.keys()),\n",
        "                label=\"Work Types\"\n",
        "            )\n",
        "\n",
        "            # checkbox for experience levels\n",
        "            exp_levels = gr.CheckboxGroup(\n",
        "                list(experience_level_mapping.keys()),\n",
        "                label=\"Experience Levels\"\n",
        "            )\n",
        "\n",
        "            # drop down for time filter\n",
        "            time_filter = gr.Dropdown(list(time_filter_mapping.keys()), label=\"Time Filter\")\n",
        "\n",
        "            # Buttons for start/stop\n",
        "            with gr.Row():\n",
        "                start_btn = gr.Button(\"Start Scraping\", variant=\"primary\")\n",
        "                stop_btn = gr.Button(\"Stop Scraping\", variant=\"secondary\")\n",
        "\n",
        "            # show status\n",
        "            status = gr.Textbox(label=\"Status\")\n",
        "\n",
        "    # show job table results\n",
        "    results = gr.DataFrame(\n",
        "        headers=[\"Position\",\"Date\",\"Work Type\",\"Level\",\"Title\",\"Company\",\"Location\",\"Skills\",\"Link\"],\n",
        "        datatype=[\"str\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\"],\n",
        "        interactive=False\n",
        "    )\n",
        "\n",
        "    # save section\n",
        "    with gr.Row():\n",
        "        # Input for file name\n",
        "        filename = gr.Textbox(label=\"Filename (optional)\", placeholder=\"my_jobs\")\n",
        "        # save button\n",
        "        save_btn = gr.Button(\"Save to CSV\", variant=\"secondary\")\n",
        "        # show save status\n",
        "        save_status = gr.Textbox(label=\"Save status\")\n",
        "\n",
        "    # === BUTTON CONNECTIONS (ALL AT SAME LEVEL) ===\n",
        "    start_btn.click(\n",
        "        run_scraper,\n",
        "        inputs=[cities, states, positions, work_types, exp_levels, time_filter],\n",
        "        outputs=[status, results]\n",
        "    )\n",
        "    stop_btn.click(\n",
        "        lambda: scraper_manager.stop_event.set(),\n",
        "        outputs=None\n",
        "    )\n",
        "    save_btn.click(\n",
        "        save_csv,\n",
        "        inputs=[results, filename],\n",
        "        outputs=save_status\n",
        "    )\n",
        "\n",
        "# === LAUNCH APP (OUTSIDE BLOCK) ===\n",
        "if __name__ == \"__main__\":\n",
        "    app.launch()"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "xb_s2jZGAB1B",
        "outputId": "3c80aca5-46c2-4bc5-8c74-4435b156706e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1a796589a177109a83.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1a796589a177109a83.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 38
    }
  ]
}