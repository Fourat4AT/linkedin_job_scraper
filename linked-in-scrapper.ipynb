{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gradio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:01:24.492413Z","iopub.execute_input":"2025-11-06T00:01:24.492704Z","iopub.status.idle":"2025-11-06T00:08:05.530823Z","shell.execute_reply.started":"2025-11-06T00:01:24.492683Z","shell.execute_reply":"2025-11-06T00:08:05.529577Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.38.1)\nRequirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.11.0)\nRequirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\nRequirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\nRequirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\nRequirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\nRequirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\nRequirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.0.0rc2)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\nRequirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c3d7dbc4e10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pydantic/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c3d7dbc5b10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pydantic/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c3d7dbc8550>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pydantic/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c3d7dbc8f10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pydantic/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c3d7dbc9d50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pydantic/\u001b[0m\u001b[33m\n\u001b[0mINFO: pip is looking at multiple versions of gradio to determine which version is compatible with other requirements. This could take a while.\n\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c3d7dd05a10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/gradio/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c3d7ddae1d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/gradio/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c3d7ddaeb50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/gradio/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c3d7dda7190>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/gradio/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c3d7dda4c90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/gradio/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pydantic<2.12,>=2.0 (from gradio) (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for pydantic<2.12,>=2.0\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip install flair \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:20:17.637435Z","iopub.execute_input":"2025-11-06T00:20:17.639876Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f2c47441990>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/flair/\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import gradio as gr  # gradio to build the web app\nimport requests      # to fetch web pages\nfrom bs4 import BeautifulSoup  # to parse html\nimport pandas as pd  # to handle data tables\nfrom datetime import datetime  # to add dates\nimport time          # for adding delays in requests\n\n# flair tools for AI skill detection\nfrom flair.models import SequenceTagger\nfrom flair.data import Sentence\n\nimport threading      # for smooth background tasks\nimport os             # to save files\nimport random         # for random delays\n\n# retry tools to handle web errors\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:11:26.932662Z","iopub.execute_input":"2025-11-06T00:11:26.933746Z","iopub.status.idle":"2025-11-06T00:11:26.960648Z","shell.execute_reply.started":"2025-11-06T00:11:26.933708Z","shell.execute_reply":"2025-11-06T00:11:26.958824Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2667486235.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# flair tools for AI skill detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequenceTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flair'"],"ename":"ModuleNotFoundError","evalue":"No module named 'flair'","output_type":"error"}],"execution_count":14},{"cell_type":"markdown","source":"**LOADING MODAL**","metadata":{}},{"cell_type":"code","source":"flair_model = SequenceTagger.load(\"kaliani/flair-ner-skill\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:11:26.963212Z","iopub.status.idle":"2025-11-06T00:11:26.963556Z","shell.execute_reply.started":"2025-11-06T00:11:26.963414Z","shell.execute_reply":"2025-11-06T00:11:26.963426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FILTRE MAPPINGS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:11:26.965394Z","iopub.status.idle":"2025-11-06T00:11:26.965809Z","shell.execute_reply.started":"2025-11-06T00:11:26.965658Z","shell.execute_reply":"2025-11-06T00:11:26.965679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Map experience levels to LinkedIn URL codes\nexperience_level_mapping = {\n    \"Internship\": \"f_E=1\",\n    \"Entry level\": \"f_E=2\",\n    \"Associate\": \"f_E=3\",\n    \"Mid-Senior level\": \"f_E=4\"\n}\n\n# Map work type to LinkedIn URL codes\nwork_type_mapping = {\n    \"On-site\": \"f_WT=1\",\n    \"Hybrid\": \"f_WT=2\",\n    \"Remote\": \"f_WT=3\"\n}\n\n# Map time filters to LinkedIn URL codes\ntime_filter_mapping = {\n    \"Past 24 hours\": \"f_TPR=r86400\",\n    \"Past week\": \"f_TPR=r604800\",\n    \"Past month\": \"f_TPR=r2592000\"\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T00:11:26.967387Z","iopub.status.idle":"2025-11-06T00:11:26.967802Z","shell.execute_reply.started":"2025-11-06T00:11:26.967602Z","shell.execute_reply":"2025-11-06T00:11:26.967621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a function to find skills in a text description\ndef get_skills(text):\n    # Turn the input text into Flair's Sentence object\n    sentence = Sentence(text)\n    \n    # Use the Flair NER model to detect skills in the sentence\n    flair_model.predict(sentence)\n    \n    # Extract the detected skills from the sentence\n    skills = [entity.text for entity in sentence.get_spans(\"ner\")]\n    \n    return skills\n\n# Example usage:\ndescription = \"I have experience in Python, TensorFlow, and data analysis.\"\nfound_skills = get_skills(description)\nprint(found_skills)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T22:09:44.181452Z","iopub.execute_input":"2025-11-07T22:09:44.181693Z","iopub.status.idle":"2025-11-07T22:09:44.278200Z","shell.execute_reply.started":"2025-11-07T22:09:44.181674Z","shell.execute_reply":"2025-11-07T22:09:44.276983Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2386792330.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Example usage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"I have experience in Python, TensorFlow, and data analysis.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mfound_skills\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_skills\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_skills\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/2386792330.py\u001b[0m in \u001b[0;36mget_skills\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_skills\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Turn the input text into Flair's Sentence object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Use the Flair NER model to detect skills in the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Sentence' is not defined"],"ename":"NameError","evalue":"name 'Sentence' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"class ScraperManager:\n    # Setup when class starts\n    def __init__(self):\n        # Flag to stop scraping\n        self.stop_event = threading.Event()\n        \n        # Empty table for job data\n        self.current_df = pd.DataFrame()\n        \n        # Lock to avoid data mix-ups\n        self.lock = threading.Lock()\n\n    # Reset for new scrape\n    def reset(self):\n        # Clear stop flag\n        self.stop_event.clear()\n        \n        # Clear job table\n        self.current_df = pd.DataFrame()\n\n    # Add one job to table\n    def add_job(self, job_data):\n        # Lock to keep data safe\n        with self.lock:\n            # Make job into tiny table\n            new_df = pd.DataFrame([job_data])\n            \n            # Add to main table\n            self.current_df = pd.concat([self.current_df, new_df], ignore_index=True)\n\n# -------------------------------\n# Create manager instance\n# -------------------------------\nscraper_manager = ScraperManager()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#define function to save jobs\ndef save_csv(df, filename=\"jobs\"):\n    try:\n        #make folder for files\n        os.makedirs(\"saved_jobs\", exist_ok=True)\n        \n        #set default name with time stamp\n        if not filename:\n            filename = f\"jobs_{int(time.time())}\"\n        \n        #build file path\n        full_path = f\"saved_jobs/{filename}.csv\"\n        \n        #save table to csv\n        df.to_csv(full_path, index=False)\n        \n        #Confirm save worked\n        return f\"Saved to {full_path}\"\n    \n    except Exception as e:\n        #show error if save fails\n        return f\"Save error: {str(e)}\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Proecss JOB FUNCTION**","metadata":{}},{"cell_type":"code","source":"def process_job(job, work_type, exp_level, position):\n    try:\n        # find job title\n        title_element = job.find('h3', class_='base-search-card__title')\n        # find company name\n        company_element = job.find('a', class_='hidden-nested-link')\n        # find location\n        loc_element = job.find('span', class_='job-search-card__location')\n        # find job link\n        link_element = job.find('a', class_='base-card__full-link')\n\n        # check all data exists\n        if not all([title_element, company_element, loc_element, link_element]):\n            return None\n\n        # clean title text\n        title = title_element.text.strip()\n        # clean company text\n        company = company_element.text.strip()\n        # clean location text\n        loc = loc_element.text.strip()\n        # clean link (remove extra bits)\n        link = link_element['href'].split('?')[0]\n\n        # setup web session with retries\n        session = requests.Session()\n        retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n        session.mount('https://',HTTPAdapter(max_retries=retries))\n        \n        # make request to job link\n        desc = \"Description not available \"\n        #empty skills list \n        skills = []\n        \n        try:\n            time.sleep(random.uniform(2,5))\n            response = session.get(\n                link,\n                headers={\n                    'User-Agent': random.choice([\n                        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)',\n                        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko)',\n                        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96'\n                    ]),\n                    \"Accept-Language\": \"en-us,en;q=0.9\"\n                },\n                timeout=10\n            )\n\n        # parse job page\n        job_soup = BeautifulSoup(response.text, 'html.parser')\n\n        # list of places to find description\n        description_selectors = [\n            'div.description__text',\n            'div.show-more-less-html__markup',\n            'div.core-section-container__content',\n            'section.core-section-container'\n        ]\n\n        \n\n        # try each description spot\n        for selector in description_selectors:\n            desc_element = job_soup.select_one(selector)\n            if desc_element:\n                # clean description text\n                desc = desc_element.get_text('\\n').strip()\n                # find skills with AI\n                skills = get_skills(desc)\n                break\n    except Exception as  e:\n        print(f\"Errror processing {link}:{str(e)}\")\n        # return job details\n        return {\n            \"Position\": position,\n            \"Date\": datetime.now().strftime('%Y-%m-%d'),\n            \"Work type\": work_type,\n            \"Level\": exp_level,\n            \"Title\": title,\n            \"Company\": company,\n            \"Location\": loc,\n            \"Link\": f\"[{link}]({link})\",\n            \"Description\": desc,\n            \"Skills\": \", \".join(skills[:5]) if skills else \"No skills detected\"\n        }\n\n    except Exception as e:\n        # log error if job fails\n        print(f\"Error packing job card: {str(e)}\")\n        return None\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**SCRAPE JOB FUNCTION******","metadata":{}},{"cell_type":"code","source":"def scrape_jobs(location, position, work_types, exp_levels, time_filter):\n    # setup web session\n    session = requests.Session()\n\n    # set retry rules\n    retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n    session.mount('https://', HTTPAdapter(max_retries=retries))\n\n    # loop through work types\n    for work_type in work_types:\n\n        # loop through experience levels\n        for exp_level in exp_levels:   \n            # stop if user says so  \n            if scraper_manager.stop_event.is_set():\n                return\n            try:      \n                # Build LinkedIn search URL \n                base_url = f\"https://www.linkedin.com/jobs/search/?keywords={position}&location={location}\"\\\n                            f\"&{work_type_mapping[work_type]}\" \\\n                            f\"&{experience_level_mapping[exp_level]}\" \\\n                            f\"&{time_filter_mapping[time_filter]}\"\\\n                            f\"&radius=0\"\n                \n                try:\n                    # Get search page\n                    response = session.get(base_url, timeout=18)\n                    soup = BeautifulSoup(response.text, 'html.parser')\n\n                    # find total job count\n                    total_jobs = int(soup.find('span', class_='results-context-header__job-count').text.replace(',',''))\n                except:\n                    total_jobs = 25\n\n                total_jobs = min(total_jobs, 100)\n                \n                for start in range(0, total_jobs, 25):\n\n                    # stop if user says so\n                    if scraper_manager.stop_event.is_set():\n                        return\n\n                    time.sleep(random.uniform(2, 5))\n\n                    url = f\"{base_url}&start={start}\"\n\n                    try:\n                        # get page \n                        response = session.get(url, timeout=10)\n                        # parse html\n                        soup = BeautifulSoup(response.text, 'html.parser')\n\n                        # find all job cards\n                        jobs = soup.find_all('div', class_='base-card')\n\n                    except Exception as e:\n                        # log page error \n                        print(f\"Failed to scrape page {start}: {str(e)}\")\n                        continue\n\n                    random.shuffle(jobs)\n\n                    for job in jobs:\n                        # stop if user says so\n                        if scraper_manager.stop_event.is_set():\n                            return\n\n                        # get job details \n                        job_data = process_job(job, work_type, exp_level, position)\n                        if job_data:\n                            # add to table\n                            scraper_manager.add_job(job_data)\n                            # update app\n                            yield\n\n            except Exception as e:\n                # log big error\n                print(f\"Scraping error: {str(e)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define function to start scraping\ndef run_scrapper(cities, states, positions, work_types, exp_levels, time_filter):\n    # clear old data\n    scraper_manager.reset()\n\n    # split cities into list\n    cities_list = [c.strip() for c in cities.split(',') if c.strip()]\n\n    # split states/countries into list\n    states_list = [s.strip() for s in states.split(',') if s.strip()]\n\n    # combine cities and states\n    locations = [f\"{city}, {state}\" for city in cities_list for state in states_list]\n\n    # positions\n    positions = [p.strip().replace(' ', '%20') for p in positions.split(',') if p.strip()]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define scrapping task\ndef worker():\n    # Loop through locations\n    for loc in locations:\n        # loop through positions\n        for pos in positions:\n            # stop if user says so\n            if scraper_manager.stop_event.is_set():\n                return\n            # <<< your scraping logic will be here later >>>\n            pass\n\nthread = threading.Thread(target=worker)\nthread.start()\n\n# update app while running\nwhile thread.is_alive():\n    # wait a bit\n    time.sleep(0.5)\n    with scraper_manager.lock:\n        # show progress and table\n        yield \"scraping in progress ...\", scraper_manager.current_df\n\n# show final status\nyield (\"scraping completed\" if not scraper_manager.stop_event.is_set() else \"scraping stopped\"), scraper_manager.current_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with gr.Blocks() as app:\n    # Add title\n    gr.Markdown(\"\"\"\n    <div style='text-align:center; color:#f67d3c; font-size:2em; font-weight:bold; margin:20px 0; padding:10px'>\n        AI-Powered LinkedIn Scraper\n    </div>\n    \"\"\")\n\n    with gr.Row():\n        with gr.Column():\n            # Input for cities\n            cities = gr.Textbox(label=\"Cities (comma-separated)\")\n\n            # Input for states\n            states = gr.Textbox(label=\"States / Countries (comma-separated)\")\n\n            # Input for positions\n            positions = gr.Textbox(label=\"Positions (comma-separated)\")\n\n            # checkbox for work types\n            work_types = gr.CheckboxGroup(\n                list(work_type_mapping.keys()),\n                label=\"Work Types\"\n            )\n\n            # checkbox for experience levels\n            exp_levels = gr.CheckboxGroup(\n                list(experience_level_mapping.keys()),\n                label=\"Experience Levels\"\n            )\n\n            # drop down for time filter\n            time_filter = gr.Dropdown(list(time_filter_mapping.keys()), label=\"Time Filter\")\n\n            # Buttons for start/stop\n            with gr.Row():\n                start_btn = gr.Button(\"Start Scraping\", variant=\"primary\")\n                stop_btn  = gr.Button(\"Stop Scraping\", variant=\"secondary\")\n\n            # show status\n            status = gr.Textbox(label=\"Status\")\n\n    # show job table results\n    results = gr.DataFrame(\n        headers=[\"Position\",\"Date\",\"Work Type\",\"Level\",\"Title\",\"Company\",\"Location\",\"Skills\",\"Link\"],\n        datatype=[\"str\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\"],\n        interactive=False\n    )\n\n    # save section\n    with gr.Row():\n        # Input for file name\n        filename   = gr.Textbox(label=\"Filename (optional)\", placeholder=\"my_jobs\")\n        # save button\n        save_btn   = gr.Button(\"Save to CSV\", variant=\"secondary\")\n        # show save status\n        save_status = gr.Textbox(label=\"Save status\")\n\n    # link start button to scraper\n    start_btn.click(\n        run_scraper,\n        inputs=[cities, states, positions, work_types, exp_levels, time_filter],\n        outputs=[status, results]\n    )\n\n    # link stop button to stop event\n    stop_btn.click(\n        lambda: scraper_manager.stop_event.set(),\n        outputs=None\n    )\n\n    # link save button to save\n    save_btn.click(\n        save_csv,\n        inputs=[results, filename],\n        outputs=save_status\n    )\n\n# run the app\nif __name__ == \"__main__\":\n    app.launch()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}